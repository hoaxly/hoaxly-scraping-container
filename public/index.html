<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2018-04-15 Sun 19:15 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>&lrm;</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="rosenstrauch" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2017 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
</head>
<body>
<div id="content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgdac6890">1. HoaxlyScrapingContainer</a>
<ul>
<li>
<ul>
<li><a href="#org1190d13">1.0.1. ReadMe for a summary about the HoaxlyScrapingContainer</a></li>
<li><a href="#org6ec9051">1.0.2. Hoaxly Crawler Components</a></li>
<li><a href="#orgc888ec9">1.0.3. Hoaxly Container Ports (and adapters)</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<div id="outline-container-orgdac6890" class="outline-2">
<h2 id="orgdac6890"><a id="ID-b2ef372c-735c-47ea-8ecb-3749ca62c06d"></a><span class="section-number-2">1</span> HoaxlyScrapingContainer</h2>
<div class="outline-text-2" id="text-1">
<div class="org-center">
<p>
Crawl whole websites or parts of a website  extracting the data you need from websites.
</p>

<p>
You can index a whole website with the web crawler module of Apache ManifoldCF.
</p>

<p>
With its Webinterface you can setup a homepage, a sitemap or a RSS-Feed as the start point and set how deep the crawl should be.
</p>

<p>
Its possible to setup rules which parts to crawl and which to exclude.
</p>

<p>
Another software for crawling a website is Scrapy.  <a href="https://scrapy.org/">https://scrapy.org/</a>
</p>
</div>
</div>
<div id="outline-container-org1190d13" class="outline-4">
<h4 id="org1190d13"><span class="section-number-4">1.0.1</span> ReadMe for a summary about the HoaxlyScrapingContainer</h4>
<div class="outline-text-4" id="text-1-0-1">
<p>
portia is an abstraction layer on top of scrapy.
that provides a ui in the browser for orchestrating spiders, portia spiders can be exported in two ways
</p>

<p>
is a portia spiders collection for crawling websites and scraping content.
</p>

<dl class="org-dl">
<dt>we create some code and call it a spider</dt><dd>a spider describes what data to get from where</dd>
</dl>
<p>
You can write scrapy spiders in python code or you can use a service like portia (portia can be selfhosted
or used on saas platform scrapycloud/hub) to build your spider in the browser and export. Portia provides you with
an additional abstraction layer on top of scrapy.
</p>


<p>
A running spider is called a crawler.
running a spider to scrape the data we care about from a source
A spider crawl can be triggered manually to fetch data once or schedule a spider to crawl on a regular basis to fetch
data continuously.
During a crawl the spider retrieves data and outputs it to a target (stdout, json files etc.)
</p>
</div>
<ol class="org-ol">
<li><a id="org8069e5d"></a>Setup<br />
<div class="outline-text-5" id="text-1-0-1-1">
<p>
Requirements
</p>

<p>
docker-2.3.0 docker-compose-1.13.0
</p>

<p>
<span class="underline">Note:</span> make sure to run this on your host.
This is needed for elasticsearch to work [4]
</p>
<pre class="example">
sudo sysctl -w vm.max_map_count=262144
</pre>
</div>
<ol class="org-ol">
<li><a id="orgd518c07"></a>step 1 is to fetch the images<br />
<div class="outline-text-6" id="text-1-0-1-1-1">
<p>
login to our registry if you have access to get at the images you need to locally build and run spiders.
</p>

<div class="org-src-container">
<pre class="src src-shell">docker login registry.acolono.net:444
docker pull registry.acolono.net:444/hoaxly/hoaxly-storage-container
docker pull registry.acolono.net:444/hoaxly/hoaxly-scrapydaemon-container
docker pull registry.acolono.net:444/hoaxly/hoaxly-scraping-container
</pre>
</div>
</div>
</li>
<li><a id="orgf7de67a"></a>Step 2 is to spin up the local instances and initialize them.<br />
<div class="outline-text-6" id="text-1-0-1-1-2">
<p>
from projectroot run
</p>
<pre class="example">
fin init
</pre>
</div>
</li>
</ol>
</li>
</ol>
</div>

<div id="outline-container-org6ec9051" class="outline-4">
<h4 id="org6ec9051"><span class="section-number-4">1.0.2</span> Hoaxly Crawler Components</h4>
<div class="outline-text-4" id="text-1-0-2">
<p>
this container repo contains:
</p>
</div>

<ol class="org-ol">
<li><a id="orgb6efb96"></a>Visual Spider Builder (Portia):<br />
<div class="outline-text-5" id="text-1-0-2-1">
<p>
a spider describes what data we want to fetch and which pages to crawl searching for that data.
</p>
</div>
<ol class="org-ol">
<li><a id="orgf378542"></a>in your browser you can visit the [webinterface of portia](<a href="http://hoaxly.docksal:9001/#/projects">http://hoaxly.docksal:9001/#/projects</a>) use this to build new spiders<br />
<div class="outline-text-6" id="text-1-0-2-1-1">
<p>
Spiders are stored in [./portia_projects](./portia_projects)
</p>

<p>
we are using [Custom Spider middleware](<a href="https://doc.scrapy.org/en/latest/topics/spider-middleware.html#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output">https://doc.scrapy.org/en/latest/topics/spider-middleware.html#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output</a>) for enriching item with scraped metadata
</p>

<p>
portia_projects/hoaxlyPortia/spidermiddleware.py
</p>
</div>
</li>
<li><a id="org85d6d3b"></a><a id="ID-f5cea585-15aa-4e87-b546-9f47bae6fee3"></a>How to create a new Spider<br />
<div class="outline-text-6" id="text-1-0-2-1-2">
</div>
<ol class="org-ol">
<li><a id="orgadc1687"></a><a id="ID-a56c5c3d-abf8-41e9-a6aa-b364160859eb"></a>create a new branch<br />
<div class="outline-text-7" id="text-1-0-2-1-2-1">

<div class="figure">
<p><img src="hoaxly.org_imgs/20180119_143931_3319QVe.png" alt="20180119_143931_3319QVe.png" />
</p>
</div>
</div>
</li>


<li><a id="org4142185"></a><a id="ID-679cbfab-c484-4f87-8b92-c913bbbbb573"></a>visit <a href="http://hoaxly.docksal:9001/#/projects/hoaxlyPortia">http://hoaxly.docksal:9001/#/projects/hoaxlyPortia</a><br />
<div class="outline-text-7" id="text-1-0-2-1-2-2">
</div>
</li>
<li><a id="orgda793d5"></a><a id="ID-3f7bab5a-8aac-4552-b3b0-102a0dfb2e79"></a>enter url you want to scrape<br />
<div class="outline-text-7" id="text-1-0-2-1-2-3">

<div class="figure">
<p><img src="hoaxly.org_imgs/20180119_144324_3319dfk.png" alt="20180119_144324_3319dfk.png" />
</p>
</div>


<div class="figure">
<p><img src="hoaxly.org_imgs/20180119_144527_3319qpq.png" alt="20180119_144527_3319qpq.png" />
</p>
</div>
</div>
</li>

<li><a id="org88b2b4b"></a><a id="ID-64beade0-a088-4413-bb21-c4ff8672ed6e"></a>visit the page where you want to start crawling through links<br />
<div class="outline-text-7" id="text-1-0-2-1-2-4">

<div class="figure">
<p><img src="hoaxly.org_imgs/20180119_144652_33193zw.png" alt="20180119_144652_33193zw.png" />
</p>
</div>
</div>
</li>
<li><a id="orgfb31369"></a><a id="ID-657c0a53-b887-4835-a8e9-f71f86be71ab"></a>create a new spider<br />
<div class="outline-text-7" id="text-1-0-2-1-2-5">

<div class="figure">
<p><img src="hoaxly.org_imgs/20180119_144716_3319E-2.png" alt="20180119_144716_3319E-2.png" />
</p>
</div>
</div>
</li>
<li><a id="orgeecbb4d"></a><a id="ID-31ce87a1-b0ff-43f5-80d1-0844381eb09c"></a>follow a link to a sample item you want to scrape<br />
<div class="outline-text-7" id="text-1-0-2-1-2-6">

<div class="figure">
<p><img src="hoaxly.org_imgs/20180119_144817_33192HG.png" alt="20180119_144817_33192HG.png" />
</p>
</div>


<div class="figure">
<p><img src="hoaxly.org_imgs/20180119_144832_3319DSM.png" alt="20180119_144832_3319DSM.png" />
</p>
</div>
</div>
</li>
<li><a id="org7a11b34"></a><a id="ID-b5e6e56a-ad67-41b1-bc4d-d3ca398d2594"></a>create a new sample anotation<br />
<div class="outline-text-7" id="text-1-0-2-1-2-7">

<div class="figure">
<p><img src="hoaxly.org_imgs/20180119_144856_3319QcS.png" alt="20180119_144856_3319QcS.png" />
</p>
</div>
</div>
</li>
<li><a id="org654af9f"></a><a id="ID-6382897c-f172-4835-bf55-0378bf06711e"></a>select the appropriate schema<br />
<div class="outline-text-7" id="text-1-0-2-1-2-8">

<div class="figure">
<p><img src="hoaxly.org_imgs/20180119_144936_3319dmY.png" alt="20180119_144936_3319dmY.png" />
</p>
</div>


<div class="figure">
<p><img src="hoaxly.org_imgs/20180119_145019_3319qwe.png" alt="20180119_145019_3319qwe.png" />
</p>
</div>
</div>
</li>
<li><a id="org41c7945"></a><a id="ID-49b3ce66-cd45-4b1f-ab8b-001de12f3e44"></a>annotate the first element by clicking on the visible project headline<br />
<div class="outline-text-7" id="text-1-0-2-1-2-9">

<div class="figure">
<p><img src="hoaxly.org_imgs/20180119_145056_331936k.png" alt="20180119_145056_331936k.png" />
</p>
</div>
</div>
</li>
<li><a id="orgcde511d"></a><a id="ID-32ff9456-8ec8-4cbf-a2aa-11734ac7a1ac"></a>select the appropriate field from schema<br />
<div class="outline-text-7" id="text-1-0-2-1-2-10">

<div class="figure">
<p><img src="hoaxly.org_imgs/20180119_145146_3319EFr.png" alt="20180119_145146_3319EFr.png" />
</p>
</div>
</div>
</li>
<li><a id="org51babf9"></a><a id="ID-4215fafd-5293-48f6-8865-f661a5266528"></a>repeat for all fields in the schema<br />
<div class="outline-text-7" id="text-1-0-2-1-2-11">

<div class="figure">
<p><img src="hoaxly.org_imgs/20180119_145238_3319RPx.png" alt="20180119_145238_3319RPx.png" />
</p>
</div>


<div class="figure">
<p><img src="hoaxly.org_imgs/20180119_145415_3319DZA.png" alt="20180119_145415_3319DZA.png" />
</p>
</div>
</div>
</li>
<li><a id="org22dfd8c"></a><a id="ID-c35b514d-29c7-47e1-90cf-a5e0fddaa3ba"></a>close sample<br />
<div class="outline-text-7" id="text-1-0-2-1-2-12">

<div class="figure">
<p><img src="hoaxly.org_imgs/20180119_145433_3319QjG.png" alt="20180119_145433_3319QjG.png" />
</p>
</div>
</div>
</li>
<li><a id="orgf0f4109"></a><a id="ID-95b6f7ff-1bb8-4451-90cc-7614939b78ab"></a>configure url crawiling schema<br />
<div class="outline-text-7" id="text-1-0-2-1-2-13">

<div class="figure">
<p><img src="hoaxly.org_imgs/20180119_145501_3319dtM.png" alt="20180119_145501_3319dtM.png" />
</p>
</div>

<p>
using regex
<img src="hoaxly.org_imgs/20180119_145607_3319q3S.png" alt="20180119_145607_3319q3S.png" />
</p>
</div>
</li>
<li><a id="orgd1be94d"></a>export spider as scrapy spider (python code)<br /></li>
<li><a id="orgd9d7a07"></a><a id="ID-f8162753-b52f-4264-a52b-f8f79a37b3ae"></a>add the new spider to the scrapy_projects directory and commit the new spider<br />
<div class="outline-text-7" id="text-1-0-2-1-2-15">

<div class="figure">
<p><img src="hoaxly.org_imgs/20180119_145722_33193BZ.png" alt="20180119_145722_33193BZ.png" />
</p>
</div>

<pre class="example">
☻ % git add scrapy_projects/hoaxlyPortia/spiders/ -p
☻ % git commit scrapy_projects/hoaxlyPortia/spiders/
</pre>

<p>
use a commit message that tells us what spider you are adding using which schema
</p>
</div>
</li>
<li><a id="org64b1b01"></a><a id="ID-b0b5e916-db45-4fa8-8e59-39a7951210d3"></a>create a merge request<br />
<div class="outline-text-7" id="text-1-0-2-1-2-16">

<div class="figure">
<p><img src="hoaxly.org_imgs/20180119_150000_3319EMf.png" alt="20180119_150000_3319EMf.png" />
</p>
</div>

<p>
assign it to someone for review
</p>



<p>
TODO: define a useful <a href="https://gitlab.acolono.net/help/user/project/description_templates">https://gitlab.acolono.net/help/user/project/description_templates</a> for spider contributions
</p>
</div>
</li>
</ol>
</li>
<li><a id="orgc48120a"></a>Running a spider<br />
<div class="outline-text-6" id="text-1-0-2-1-3">
<p>
This is useful for testing your spider locally before using it to retrieve data regularly.
</p>

<p>
For portia spiders: portiacrawl command <sup><a id="fnr.1" class="footref" href="#fn.1">1</a></sup>
For spiders created programmatically: scrapy crawl cli command 
</p>


<p>
you will get a list of spiders if you run this command
</p>
<pre class="example">

docker exec portia  &lt;PROJECT_PATH&gt; [SPIDER] [OPTIONS]
docker exec portia portiacrawl /app/data/projects/hoaxlyPortia
</pre>
<p>
for example try
</p>
<pre class="example">

docker exec portia portiacrawl /app/data/projects/hoaxlyPortia www.snopes.com -o /app/data/example-output/snopes-output.json
--settings=hoaxly

</pre>

<p>
the more lowlevel command looks like
</p>
<pre class="example">

scrapy crawl -s PROJECT_DIR=./ -s SPIDER_MANAGER_CLASS=slybot.spidermanager.SlybotSpiderManager snopes.com

</pre>

<p>
you can also locally deploy exported spiders to the scrapingdaemon and schedule a run there to test what would happen in production environment
there is a cli container supplied so you dont need to install any dependencies on your host
</p>


<pre class="example">

☻ % docker exec -ti cli /bin/bash

</pre>

<p>
then you are in container and can
</p>
<pre class="example">

scrapyd-client deploy local
scrapyd-client -t http://scrapydaemon.hoaxly.docksal:6800/schedule -p HoaxlyPortia climatefeedback.org

</pre>
<p>
and view your results in the storage container:
</p>

<p>
<a href="http://elastic.hoaxly.docksal:9200/hoaxly/_search">http://elastic.hoaxly.docksal:9200/hoaxly/_search</a>
</p>
</div>
</li>
</ol>
</li>



<li><a id="org4182255"></a>Deploy to Crawling service (scrapyd)<br />
<div class="outline-text-5" id="text-1-0-2-2">
<p>
is a daemon that can be started to schedule runs
</p>


<ul class="org-ul">
<li><a href="https://doc.scrapy.org/en/latest/index.html">https://doc.scrapy.org/en/latest/index.html</a></li>
<li><a href="http://scrapyd.readthedocs.io/en/latest/">http://scrapyd.readthedocs.io/en/latest/</a></li>
</ul>


<p>
configure your live instance hostname in <a href="./scrapy_projects/scrapy.cfg">scrapy.cfg</a>
once you tested everything locally you can deploy to live scrapyd and scheduling crawls using [scrapyd-client](<a href="https://github.com/scrapy/scrapyd-client">https://github.com/scrapy/scrapyd-client</a>)
</p>
<pre class="example">
docker exec -ti cli bash
scrapyd-deploy live
</pre>
<p>
once deployed you can interact directly with scrapyd through the webapi, either using the client
</p>

<pre class="example">
docker exec -ti cli bash
scrapyd-client -t https://htaccessusername:htaccesspassword@scrapyd.hoax.ly/ schedule -p hoaxlyPortia climatefeedback.org
</pre>

<p>
or from anywhere else.
</p>

<p>
curl <a href="https://htaccessusername:htaccesspassword@scrapyd.hoax.ly/schedule.json">https://htaccessusername:htaccesspassword@scrapyd.hoax.ly/schedule.json</a> -d project=HoaxlyPortia -d spider=www.theskepticsguide.org
curl <a href="https://htaccessusername:htaccesspassword@scrapyd.hoax.ly/listprojects.json">https://htaccessusername:htaccesspassword@scrapyd.hoax.ly/listprojects.json</a>
curl <a href="https://htaccessusername:htaccesspassword@scrapyd.hoax.ly/listspiders.json?project=HoaxlyPortia">https://htaccessusername:htaccesspassword@scrapyd.hoax.ly/listspiders.json?project=HoaxlyPortia</a>
</p>


<p>
A crawl can be scheduled to run regularly by deploying it to a dedicated server.
</p>

<p>
for portia spiders deployment should work normally but currently requires a workaround in our settings
</p>
</div>
</li>

<li><a id="org7a2ee22"></a>Settings<br />
<div class="outline-text-5" id="text-1-0-2-3">
<p>
<a href="./scrapy_projects/Hoaxlyspiders/settings.py">scrapy spider settings</a>
</p>

<p>
<a href="./portia_projects/Hoaxlyspiders/spiders/settings.py">portiaproject settings</a>
</p>
</div>
</li>
<li><a id="org1cece90"></a>Helpers middleware<br />
<div class="outline-text-5" id="text-1-0-2-4">
<p>
the <a href="./portia_projects/packages">HoaxlyHelpers Middleware package contains things that</a> have been moved into their own helper package and are installed in both the spiderbreeder and runner containers
</p>


<p>
by default a running spider just outputs to screen or files.
by configuring a pipeline we can define where the data is also sent.
</p>

<p>
if you want to store everthing you can just pipe it to a storage service.
in most cases you will want to process the data beforehand in order to e.g. filter out unnecessary parts
</p>
<dl class="org-dl">
<dt>to do that we use middleware</dt><dd>by configuring middleware we can manipulate the data or spider</dd>

<dt>(no term)</dt><dd>pipeline for storing in elasticsearch</dd>
<dt>microdata middlware</dt><dd>extract microdata along with visual data</dd>
<dt>index &amp; type pipelines</dt><dd>compatibility layer to make bulk uploading to es work</dd>
</dl>
</div>
</li>



<li><a id="orgee7d535"></a>docker container in registry<br />
<div class="outline-text-5" id="text-1-0-2-5">
<p>
to be used in local dev and in production
</p>
<div class="org-src-container">
<pre class="src src-dockerfile"><span style="color: #20b2aa; font-weight: bold;">FROM</span> scrapinghub/portia


<span style="color: #cd853f;"># </span><span style="color: #cd853f;">the file with our requirements</span>
<span style="color: #20b2aa; font-weight: bold;">COPY</span> portia_projects/requirements.txt .
<span style="color: #cd853f;"># </span><span style="color: #cd853f;">our helper package</span>
<span style="color: #20b2aa; font-weight: bold;">COPY</span> portia_projects/packages /app/data/projects/packages
<span style="color: #cd853f;"># </span><span style="color: #cd853f;">our current spiders</span>
<span style="color: #20b2aa; font-weight: bold;">COPY</span> portia_projects/hoaxlyPortia /app/data/projects/hoaxlyPortia


<span style="color: #cd853f;"># </span><span style="color: #cd853f;">and our own requirements</span>
<span style="color: #20b2aa; font-weight: bold;">RUN</span> pip install  --no-cache-dir -r requirements.txt
<span style="color: #cd853f;"># </span><span style="color: #cd853f;">finally our own helperPackage</span>
<span style="color: #20b2aa; font-weight: bold;">RUN</span> pip install -e /app/data/projects/packages

</pre>
</div>


<p>
we are having this conviniently built by our gitlab ci bot
</p>
<div class="org-src-container">
<pre class="src src-yaml">
<span style="color: #9acd32;">image</span>: tmaier/docker-compose:17.09
<span style="color: #9acd32;">services</span>:
  - docker:17.09-dind


<span style="color: #9acd32;">stages</span>:
- build
- release


<span style="color: #9acd32;">variables</span>:
  <span style="color: #9acd32;">CONTAINER_TEST_IMAGE</span>: $CI_REGISTRY_IMAGE:$CI_COMMIT_REF_SLUG
  <span style="color: #9acd32;">CONTAINER_RELEASE_IMAGE</span>: $CI_REGISTRY_IMAGE:latest

<span style="color: #9acd32;">before_script</span>:
  - docker login -u gitlab-ci-token -p $CI_BUILD_TOKEN registry.acolono.net:444

<span style="color: #9acd32;">build</span>:
  <span style="color: #9acd32;">stage</span>: build
  <span style="color: #9acd32;">script</span>:
    - docker build --pull -t $CONTAINER_TEST_IMAGE .
    - docker push $CONTAINER_TEST_IMAGE


<span style="color: #9acd32;">release-image</span>:
  <span style="color: #9acd32;">stage</span>: release
  <span style="color: #9acd32;">script</span>:
    - docker pull $CONTAINER_TEST_IMAGE
    - docker tag $CONTAINER_TEST_IMAGE $CONTAINER_RELEASE_IMAGE
    - docker push $CONTAINER_RELEASE_IMAGE
  <span style="color: #9acd32;">only</span>:
    - master

</pre>
</div>
</div>
</li>
</ol>
</div>

<div id="outline-container-orgc888ec9" class="outline-4">
<h4 id="orgc888ec9"><span class="section-number-4">1.0.3</span> Hoaxly Container Ports (and adapters)</h4>
<div class="outline-text-4" id="text-1-0-3">
<p>
to talk to the other hoaxly containers
</p>
</div>
<ol class="org-ol">
<li><a id="orgc0dcd15"></a>exposes port 6800 to allow scheduling spiders<br />
<div class="outline-text-5" id="text-1-0-3-1">
<p>
scrapyd, if running, can be interacted with 
</p>
</div>
</li>
<li><a id="orgedcf18d"></a>uses port 9200 an 9300 to read from Storage Container via<br />
<div class="outline-text-5" id="text-1-0-3-2">
<p>
Elasticsearch via scrapyelasticsearch python library [3]
</p>
</div>
</li>
</ol>
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1">1</a></sup> <div class="footpara"><p class="footpara">
<a href="http://portia.readthedocs.io/en/latest/spiders.html#running-a-spider">http://portia.readthedocs.io/en/latest/spiders.html#running-a-spider</a>
</p></div></div>


</div>
</div></div>
<div id="postamble" class="status">
<p class="author">Author: rosenstrauch</p>
<p class="date">Created: 2018-04-15 Sun 19:15</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
