* HoaxlyScrapingContainer
  :PROPERTIES:
  :ID:       b2ef372c-735c-47ea-8ecb-3749ca62c06d
  :END:

  #+BEGIN_CENTER
  Crawl whole websites or parts of a website extracting the data you need from websites.
  We use Scrapy https://scrapy.org/ for our website crawling needs.


  #+END_CENTER
*** ReadMe for a summary about the HoaxlySpiderbreederContainer
    A spider describes what data to get from where. You can write scrapy spiders in python code or you can use a tool like Portia.
    Portia is an abstraction layer on top of scrapy that provides a UI in the browser for creating spiders.
    Portia spiders can be exported in two ways (portia and scrapy)
    This is a portia spiders collection for crawling websites and scraping content.
       
    A running spider is called a crawler.
    A spider crawl can be triggered manually to fetch data once or scheduled to crawl on a regular basis to fetch
    data continuously. During a crawl the spider retrieves data and outputs it to a target (stdout, json files etc.)
**** Setup

     _Requirements:_

     - docker-2.3.0
     - docker-compose-1.13.0

     _Note:_ make sure to run this on your host.
     This is needed for elasticsearch to work [4]
     #+BEGIN_EXAMPLE
     sudo sysctl -w vm.max_map_count=262144
     #+END_EXAMPLE
***** Step 1: Fetch the images

      Login to our registry (using your gitlab credentials) to get at the images that you need in order to locally build and run spiders.

      #+BEGIN_SRC shell :eval never-export
        docker login registry.acolono.net:444
        docker pull registry.acolono.net:444/hoaxly/hoaxly-storage-container
        docker pull registry.acolono.net:444/hoaxly/hoaxly-scrapydaemon-container
        docker pull registry.acolono.net:444/hoaxly/hoaxly-scraping-container
      #+END_SRC
***** Step 2: Spin up the local instances and initialize them.
      In your project's rootfolder, run:
      #+BEGIN_EXAMPLE
        fin init
      #+END_EXAMPLE

*** Hoaxly Crawler Components

    This repo contains:

**** Visual Spider Builder (Portia):
     A spider describes what data we want to fetch and which pages to crawl searching for that data.
***** Visit the [webinterface of portia](http://hoaxly.docksal:9001/#/projects): use this to build new spiders

      Spiders are stored in [[file:portia_projects][hoaxly-scraping-container/portia_projects]]

***** How to create a new Spider
      :PROPERTIES:
      :ID:       f5cea585-15aa-4e87-b546-9f47bae6fee3
      :END:
****** create a new branch
       :PROPERTIES:
       :ID:       a56c5c3d-abf8-41e9-a6aa-b364160859eb
       :END:


       [[file:hoaxly.org_imgs/20180119_143931_3319QVe.png]]


****** visit http://hoaxly.docksal:9001/#/projects/hoaxlyPortia
       :PROPERTIES:
       :ID:       679cbfab-c484-4f87-8b92-c913bbbbb573
       :END:
****** enter url you want to scrape
       :PROPERTIES:
       :ID:       3f7bab5a-8aac-4552-b3b0-102a0dfb2e79
       :END:

       [[file:hoaxly.org_imgs/20180119_144527_3319qpq.png]]

****** Using the portia interface visit the page where you want to start crawling through links
       :PROPERTIES:
       :ID:       64beade0-a088-4413-bb21-c4ff8672ed6e
       :END:

       [[file:hoaxly.org_imgs/20180119_144652_33193zw.png]]
****** create a new spider
       :PROPERTIES:
       :ID:       657c0a53-b887-4835-a8e9-f71f86be71ab
       :END:

       [[file:hoaxly.org_imgs/20180119_144716_3319E-2.png]]
****** follow a link to a sample item you want to scrape
       :PROPERTIES:
       :ID:       31ce87a1-b0ff-43f5-80d1-0844381eb09c
       :END:
       [[file:hoaxly.org_imgs/20180119_144817_33192HG.png]]

       [[file:hoaxly.org_imgs/20180119_144832_3319DSM.png]]
****** create a new sample annotation
       :PROPERTIES:
       :ID:       b5e6e56a-ad67-41b1-bc4d-d3ca398d2594
       :END:
       [[file:hoaxly.org_imgs/20180119_144856_3319QcS.png]]
****** select the appropriate schema (hoaxly)
       :PROPERTIES:
       :ID:       6382897c-f172-4835-bf55-0378bf06711e
       :END:
       TODO: screenshot of new schema

       [[file:hoaxly.org_imgs/20180119_144936_3319dmY.png]]

       [[file:hoaxly.org_imgs/20180119_145019_3319qwe.png]]
****** annotate the first element by clicking on the visible project headline
       :PROPERTIES:
       :ID:       49b3ce66-cd45-4b1f-ab8b-001de12f3e44
       :END:

       [[file:hoaxly.org_imgs/20180119_145056_331936k.png]]
****** select the appropriate field from schema
       :PROPERTIES:
       :ID:       32ff9456-8ec8-4cbf-a2aa-11734ac7a1ac
       :END:
       [[file:hoaxly.org_imgs/20180119_145146_3319EFr.png]]
****** repeat for all fields in the schema
       :PROPERTIES:
       :ID:       4215fafd-5293-48f6-8865-f661a5266528
       :END:
       [[file:hoaxly.org_imgs/20180119_145238_3319RPx.png]]

       [[file:hoaxly.org_imgs/20180119_145415_3319DZA.png]]
****** close sample
       :PROPERTIES:
       :ID:       c35b514d-29c7-47e1-90cf-a5e0fddaa3ba
       :END:
       [[file:hoaxly.org_imgs/20180119_145433_3319QjG.png]]
****** configure url crawling schema
       :PROPERTIES:
       :ID:       95b6f7ff-1bb8-4451-90cc-7614939b78ab
       :END:
       [[file:hoaxly.org_imgs/20180119_145501_3319dtM.png]]

       using regex
       [[file:hoaxly.org_imgs/20180119_145607_3319q3S.png]]
****** export spider as scrapy spider (python code)
****** add the new spider to the scrapy_projects directory and commit the new spider
       :PROPERTIES:
       :ID:       f8162753-b52f-4264-a52b-f8f79a37b3ae
       :END:
       [[file:hoaxly.org_imgs/20180119_145722_33193BZ.png]]

       #+BEGIN_EXAMPLE
       ☻ % git add scrapy_projects/hoaxlyPortia/spiders/ -p
       ☻ % git commit scrapy_projects/hoaxlyPortia/spiders/
       #+END_EXAMPLE

       use a commit message that tells us what spider you are adding using which schema
****** create a merge request
       :PROPERTIES:
       :ID:       b0b5e916-db45-4fa8-8e59-39a7951210d3
       :END:
       [[file:hoaxly.org_imgs/20180119_150000_3319EMf.png]]

       assign it to someone for review



       TODO: define a useful https://gitlab.acolono.net/help/user/project/description_templates for spider contributions
***** Running a spider

      This is useful for testing your spider locally before using it to retrieve data regularly.

      For portia spiders: portiacrawl command [fn:1]
      For spiders created programmatically: scrapy crawl cli command 


      you will get a list of spiders if you run this command
      #+BEGIN_EXAMPLE

             docker exec portia portiacrawl  <PROJECT_PATH> [SPIDER] [OPTIONS]
             docker exec portia portiacrawl /app/data/projects/Hoaxlyspiders
      #+END_EXAMPLE
      For example, to run the climatefeedback.org crawler and save its output into /app/data/example-output/output.json using the hoaxly settings,
      you would run:
      #+BEGIN_EXAMPLE

             docker exec portia portiacrawl /app/data/projects/Hoaxlyspiders climatefeedback.org -o /app/data/example-output/output.json
             --settings=hoaxly

      #+END_EXAMPLE

      the more lowlevel command using scrapy looks like
      #+BEGIN_EXAMPLE

             scrapy crawl -s PROJECT_DIR=./ -s SPIDER_MANAGER_CLASS=slybot.spidermanager.SlybotSpiderManager snopes.com

      #+END_EXAMPLE

      You can also locally deploy exported spiders to the scrapingdaemon and schedule a run there to test what would happen in production environment
      there is a cli container supplied so you dont need to install any dependencies on your host

      Run:
      #+BEGIN_EXAMPLE

              ☻ % docker exec -ti cli /bin/bash

      #+END_EXAMPLE

      Now you are in container and can
      #+BEGIN_EXAMPLE

      scrapyd-client deploy local
      scrapyd-client -t http://scrapydaemon.hoaxly.docksal:6800/schedule -p HoaxlyPortia climatefeedback.org

      #+END_EXAMPLE
      and view your results in the storage container:

      http://elastic.hoaxly.docksal:9200/hoaxly/_search



**** Deploy to Crawling service (scrapyd)

     Scrapyd is a daemon that can be started to schedule runs


     - https://doc.scrapy.org/en/latest/index.html
     - http://scrapyd.readthedocs.io/en/latest/


     configure your live instance hostname in [[./scrapy_projects/scrapy.cfg][scrapy.cfg]]
     once you tested everything locally you can deploy to live scrapyd and schedule crawls using [scrapyd-client](https://github.com/scrapy/scrapyd-client)
     #+BEGIN_EXAMPLE
     docker exec -ti cli bash
     scrapyd-deploy live
     #+END_EXAMPLE
     once deployed you can interact directly with scrapyd through the webapi, either using the client

     #+BEGIN_EXAMPLE
     docker exec -ti cli bash
     scrapyd-client -t https://htaccessusername:htaccesspassword@scrapyd.hoax.ly/ schedule -p hoaxlyPortia climatefeedback.org
     #+END_EXAMPLE

     or from anywhere else.

     curl https://htaccessusername:htaccesspassword@scrapyd.hoax.ly/schedule.json -d project=HoaxlyPortia -d spider=www.theskepticsguide.org
     curl https://htaccessusername:htaccesspassword@scrapyd.hoax.ly/listprojects.json
     curl https://htaccessusername:htaccesspassword@scrapyd.hoax.ly/listspiders.json?project=HoaxlyPortia


     A crawl can be scheduled to run regularly by deploying it to a dedicated server.

     for portia spiders deployment should work normally but currently requires a workaround in our settings

**** Settings
     we are configuring our project in [[./scrapy_projects/Hoaxlyspiders/settings.py][scrapy spider settings]] and [[./portia_projects/Hoaxlyspiders/spiders/settings.py][portiaproject settings]]
**** Helpers middleware
     the [[./portia_projects/packages][HoaxlyHelpers Middleware package contains things that]] have been moved into their own helper package and are installed in both the spiderbreeder and runner containers

      Hoaxly uses [Custom Spider middleware](https://doc.scrapy.org/en/latest/topics/spider-middleware.html#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output) for enriching items with scraped metadata
       [[file:portia_projects/packages/hoaxlyHelpers/mymiddleware.py][spidermiddleware

     by default a running spider just outputs to screen or files.
     by configuring a pipeline we can define where the data is also sent.

     if you want to store everthing you can just pipe it to a storage service.
     in most cases you will want to process the data beforehand in order to e.g. filter out unnecessary parts
     - to do that we use middleware :: by configuring middleware we can manipulate the data or spider

     - pipeline for storing in elasticsearch
     - microdata middlware :: extract microdata along with visual data
     - index & type pipelines :: compatibility layer to make bulk uploading to es work



**** Distributed as Docker container registry
     to be used in local dev and in production
     #+BEGIN_SRC dockerfile :tangle Dockerfile
FROM scrapinghub/portia

# the file with our requirements
COPY portia_projects/requirements.txt .
# our helper package
COPY portia_projects/packages /app/data/projects/packages
# our current spiders
COPY portia_projects/hoaxlyPortia /app/data/projects/hoaxlyPortia


# and our own requirements
RUN pip install  --no-cache-dir -r requirements.txt
# finally our own helperPackage
RUN pip install -e /app/data/projects/packages

     #+END_SRC
     we are having our container conviniently built by our gitlab ci bot
     #+BEGIN_SRC yaml :tangle .gitlab-ci.yml
image: tmaier/docker-compose:17.09
services:
  - docker:17.09-dind

stages:
- build
- release

variables:
  CONTAINER_TEST_IMAGE: $CI_REGISTRY_IMAGE:$CI_COMMIT_REF_SLUG
  CONTAINER_RELEASE_IMAGE: $CI_REGISTRY_IMAGE:latest

before_script:
  - docker login -u gitlab-ci-token -p $CI_BUILD_TOKEN registry.acolono.net:444

build:
  stage: build
  script:
    - docker build --pull -t $CONTAINER_TEST_IMAGE .
    - docker push $CONTAINER_TEST_IMAGE

release-image:
  stage: release
  script:
    - docker pull $CONTAINER_TEST_IMAGE
    - docker tag $CONTAINER_TEST_IMAGE $CONTAINER_RELEASE_IMAGE
    - docker push $CONTAINER_RELEASE_IMAGE
  only:
    - master

pages:
  stage: release
  script:
  - echo 'Nothing to do...'
  artifacts:
    paths:
    - public
  only:
  - master

     #+END_SRC

*** Hoaxly Container Ports (and adapters)
    to talk to the other hoaxly containers
**** the spiderbuilder is exposed on 9001
**** the spiderrunner 
***** exposes port 6800 to allow scheduling spiders
      scrapyd, if running, can be interacted with 
***** tries to use port 9200 an 9300 to write to Storage Container via
      Elasticsearch via scrapyelasticsearch python library   
* Footnotes

[fn:1] http://portia.readthedocs.io/en/latest/spiders.html#running-a-spider


