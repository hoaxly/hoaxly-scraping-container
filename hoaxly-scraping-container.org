* HoaxlyScrapingContainer
  :PROPERTIES:
  :ID:       b2ef372c-735c-47ea-8ecb-3749ca62c06d
  :END:

  #+BEGIN_CENTER
  Crawl whole websites or parts of a website  extracting the data you need from websites.

  You can index a whole website with the web crawler module of Apache ManifoldCF.

  With its Webinterface you can setup a homepage, a sitemap or a RSS-Feed as the start point and set how deep the crawl should be.

  Its possible to setup rules which parts to crawl and which to exclude.

  Another software for crawling a website is Scrapy.  https://scrapy.org/


  #+END_CENTER
*** ReadMe for a summary about the HoaxlyScrapingContainer
    portia is an abstraction layer on top of scrapy.
    that provides a ui in the browser for orchestrating spiders, portia spiders can be exported in two ways

    is a portia spiders collection for crawling websites and scraping content.

    - we create some code and call it a spider :: a spider describes what data to get from where
    You can write scrapy spiders in python code or you can use a service like portia (portia can be selfhosted
    or used on saas platform scrapycloud/hub) to build your spider in the browser and export. Portia provides you with
    an additional abstraction layer on top of scrapy.


    A running spider is called a crawler.
    running a spider to scrape the data we care about from a source
    A spider crawl can be triggered manually to fetch data once or schedule a spider to crawl on a regular basis to fetch
    data continuously.
    During a crawl the spider retrieves data and outputs it to a target (stdout, json files etc.)
**** Setup

     Requirements

     docker-2.3.0 docker-compose-1.13.0

     _Note:_ make sure to run this on your host.
     This is needed for elasticsearch to work [4]
#+BEGIN_EXAMPLE
sudo sysctl -w vm.max_map_count=262144
#+END_EXAMPLE
***** step 1 is to fetch the images

      login to our registry if you have access to get at the images you need to locally build and run spiders.

      #+BEGIN_SRC shell :eval never-export
 docker login registry.acolono.net:444
 docker pull registry.acolono.net:444/hoaxly/hoaxly-storage-container
 docker pull registry.acolono.net:444/hoaxly/hoaxly-scrapydaemon-container
 docker pull registry.acolono.net:444/hoaxly/hoaxly-scraping-container
      #+END_SRC
***** Step 2 is to spin up the local instances and initialize them.
      from projectroot run
#+BEGIN_EXAMPLE
fin init
#+END_EXAMPLE

*** Hoaxly Crawler Components

    this container repo contains:

**** Visual Spider Builder (Portia):
     a spider describes what data we want to fetch and which pages to crawl searching for that data.
***** in your browser you can visit the [webinterface of portia](http://hoaxly.docksal:9001/#/projects) use this to build new spiders

      Spiders are stored in [./portia_projects](./portia_projects)

      we are using [Custom Spider middleware](https://doc.scrapy.org/en/latest/topics/spider-middleware.html#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output) for enriching item with scraped metadata

      portia_projects/hoaxlyPortia/spidermiddleware.py
***** How to create a new Spider
      :PROPERTIES:
      :ID:       f5cea585-15aa-4e87-b546-9f47bae6fee3
      :END:
****** create a new branch
       :PROPERTIES:
       :ID:       a56c5c3d-abf8-41e9-a6aa-b364160859eb
       :END:

       [[file:hoaxly.org_imgs/20180119_143931_3319QVe.png]]


****** visit http://hoaxly.docksal:9001/#/projects/hoaxlyPortia
       :PROPERTIES:
       :ID:       679cbfab-c484-4f87-8b92-c913bbbbb573
       :END:
****** enter url you want to scrape
       :PROPERTIES:
       :ID:       3f7bab5a-8aac-4552-b3b0-102a0dfb2e79
       :END:
       [[file:hoaxly.org_imgs/20180119_144324_3319dfk.png]]

       [[file:hoaxly.org_imgs/20180119_144527_3319qpq.png]]

****** visit the page where you want to start crawling through links
       :PROPERTIES:
       :ID:       64beade0-a088-4413-bb21-c4ff8672ed6e
       :END:

       [[file:hoaxly.org_imgs/20180119_144652_33193zw.png]]
****** create a new spider
       :PROPERTIES:
       :ID:       657c0a53-b887-4835-a8e9-f71f86be71ab
       :END:

       [[file:hoaxly.org_imgs/20180119_144716_3319E-2.png]]
****** follow a link to a sample item you want to scrape
       :PROPERTIES:
       :ID:       31ce87a1-b0ff-43f5-80d1-0844381eb09c
       :END:
       [[file:hoaxly.org_imgs/20180119_144817_33192HG.png]]

       [[file:hoaxly.org_imgs/20180119_144832_3319DSM.png]]
****** create a new sample anotation
       :PROPERTIES:
       :ID:       b5e6e56a-ad67-41b1-bc4d-d3ca398d2594
       :END:
       [[file:hoaxly.org_imgs/20180119_144856_3319QcS.png]]
****** select the appropriate schema
       :PROPERTIES:
       :ID:       6382897c-f172-4835-bf55-0378bf06711e
       :END:

       [[file:hoaxly.org_imgs/20180119_144936_3319dmY.png]]

       [[file:hoaxly.org_imgs/20180119_145019_3319qwe.png]]
****** annotate the first element by clicking on the visible project headline
       :PROPERTIES:
       :ID:       49b3ce66-cd45-4b1f-ab8b-001de12f3e44
       :END:

       [[file:hoaxly.org_imgs/20180119_145056_331936k.png]]
****** select the appropriate field from schema
       :PROPERTIES:
       :ID:       32ff9456-8ec8-4cbf-a2aa-11734ac7a1ac
       :END:
       [[file:hoaxly.org_imgs/20180119_145146_3319EFr.png]]
****** repeat for all fields in the schema
       :PROPERTIES:
       :ID:       4215fafd-5293-48f6-8865-f661a5266528
       :END:
       [[file:hoaxly.org_imgs/20180119_145238_3319RPx.png]]

       [[file:hoaxly.org_imgs/20180119_145415_3319DZA.png]]
****** close sample
       :PROPERTIES:
       :ID:       c35b514d-29c7-47e1-90cf-a5e0fddaa3ba
       :END:
       [[file:hoaxly.org_imgs/20180119_145433_3319QjG.png]]
****** configure url crawiling schema
       :PROPERTIES:
       :ID:       95b6f7ff-1bb8-4451-90cc-7614939b78ab
       :END:
       [[file:hoaxly.org_imgs/20180119_145501_3319dtM.png]]

       using regex
       [[file:hoaxly.org_imgs/20180119_145607_3319q3S.png]]
****** export spider as scrapy spider (python code)
****** add the new spider to the scrapy_projects directory and commit the new spider
       :PROPERTIES:
       :ID:       f8162753-b52f-4264-a52b-f8f79a37b3ae
       :END:
       [[file:hoaxly.org_imgs/20180119_145722_33193BZ.png]]

       #+BEGIN_EXAMPLE
       ☻ % git add scrapy_projects/hoaxlyPortia/spiders/ -p
       ☻ % git commit scrapy_projects/hoaxlyPortia/spiders/
       #+END_EXAMPLE

       use a commit message that tells us what spider you are adding using which schema
****** create a merge request
       :PROPERTIES:
       :ID:       b0b5e916-db45-4fa8-8e59-39a7951210d3
       :END:
       [[file:hoaxly.org_imgs/20180119_150000_3319EMf.png]]

       assign it to someone for review



       TODO: define a useful https://gitlab.acolono.net/help/user/project/description_templates for spider contributions
***** Running a spider

      This is useful for testing your spider locally before using it to retrieve data regularly.

      For portia spiders: portiacrawl command [fn:1]
      For spiders created programmatically: scrapy crawl cli command 


      you will get a list of spiders if you run this command
      #+BEGIN_EXAMPLE

             docker exec portia  <PROJECT_PATH> [SPIDER] [OPTIONS]
             docker exec portia portiacrawl /app/data/projects/hoaxlyPortia
      #+END_EXAMPLE
      for example try
      #+BEGIN_EXAMPLE

             docker exec portia portiacrawl /app/data/projects/hoaxlyPortia www.snopes.com -o /app/data/example-output/snopes-output.json
             --settings=hoaxly

      #+END_EXAMPLE

      the more lowlevel command looks like
      #+BEGIN_EXAMPLE

             scrapy crawl -s PROJECT_DIR=./ -s SPIDER_MANAGER_CLASS=slybot.spidermanager.SlybotSpiderManager snopes.com

      #+END_EXAMPLE

      you can also locally deploy exported spiders to the scrapingdaemon and schedule a run there to test what would happen in production environment
      there is a cli container supplied so you dont need to install any dependencies on your host


      #+BEGIN_EXAMPLE

              ☻ % docker exec -ti cli /bin/bash

      #+END_EXAMPLE

      then you are in container and can
      #+BEGIN_EXAMPLE

      scrapyd-client deploy local
      scrapyd-client -t http://scrapydaemon.hoaxly.docksal:6800/schedule -p HoaxlyPortia climatefeedback.org

      #+END_EXAMPLE
      and view your results in the storage container:

      http://elastic.hoaxly.docksal:9200/hoaxly/_search



**** Deploy to Crawling service (scrapyd)

     is a daemon that can be started to schedule runs


     - https://doc.scrapy.org/en/latest/index.html
     - http://scrapyd.readthedocs.io/en/latest/


     configure your live instance hostname in [[./scrapy_projects/scrapy.cfg][scrapy.cfg]]
     once you tested everything locally you can deploy to live scrapyd and scheduling crawls using [scrapyd-client](https://github.com/scrapy/scrapyd-client)
     #+BEGIN_EXAMPLE
     docker exec -ti cli bash
     scrapyd-deploy live
     #+END_EXAMPLE
     once deployed you can interact directly with scrapyd through the webapi, either using the client

     #+BEGIN_EXAMPLE
     docker exec -ti cli bash
     scrapyd-client -t https://htaccessusername:htaccesspassword@scrapyd.hoax.ly/ schedule -p hoaxlyPortia climatefeedback.org
     #+END_EXAMPLE

     or from anywhere else.

     curl https://htaccessusername:htaccesspassword@scrapyd.hoax.ly/schedule.json -d project=HoaxlyPortia -d spider=www.theskepticsguide.org
     curl https://htaccessusername:htaccesspassword@scrapyd.hoax.ly/listprojects.json
     curl https://htaccessusername:htaccesspassword@scrapyd.hoax.ly/listspiders.json?project=HoaxlyPortia


     A crawl can be scheduled to run regularly by deploying it to a dedicated server.

     for portia spiders deployment should work normally but currently requires a workaround in our settings

**** Settings
[[./scrapy_projects/Hoaxlyspiders/settings.py][scrapy spider settings]]

[[./portia_projects/Hoaxlyspiders/spiders/settings.py][portiaproject settings]]
**** Helpers middleware
     the [[./portia_projects/packages][HoaxlyHelpers Middleware package contains things that]] have been moved into their own helper package and are installed in both the spiderbreeder and runner containers


     by default a running spider just outputs to screen or files.
     by configuring a pipeline we can define where the data is also sent.

     if you want to store everthing you can just pipe it to a storage service.
     in most cases you will want to process the data beforehand in order to e.g. filter out unnecessary parts
     - to do that we use middleware :: by configuring middleware we can manipulate the data or spider

     - pipeline for storing in elasticsearch
     - microdata middlware :: extract microdata along with visual data
     - index & type pipelines :: compatibility layer to make bulk uploading to es work



**** docker container in registry
     to be used in local dev and in production
#+BEGIN_SRC dockerfile :tangle Dockerfile
FROM scrapinghub/portia


# the file with our requirements
COPY portia_projects/requirements.txt .
# our helper package
COPY portia_projects/packages /app/data/projects/packages
# our current spiders
COPY portia_projects/hoaxlyPortia /app/data/projects/hoaxlyPortia


# and our own requirements
RUN pip install  --no-cache-dir -r requirements.txt
# finally our own helperPackage
RUN pip install -e /app/data/projects/packages

#+END_SRC


we are having this conviniently built by our gitlab ci bot
#+BEGIN_SRC yaml :tangle .gitlab-ci.yml

image: tmaier/docker-compose:17.09
services:
  - docker:17.09-dind


stages:
- build
- release


variables:
  CONTAINER_TEST_IMAGE: $CI_REGISTRY_IMAGE:$CI_COMMIT_REF_SLUG
  CONTAINER_RELEASE_IMAGE: $CI_REGISTRY_IMAGE:latest

before_script:
  - docker login -u gitlab-ci-token -p $CI_BUILD_TOKEN registry.acolono.net:444

build:
  stage: build
  script:
    - docker build --pull -t $CONTAINER_TEST_IMAGE .
    - docker push $CONTAINER_TEST_IMAGE


release-image:
  stage: release
  script:
    - docker pull $CONTAINER_TEST_IMAGE
    - docker tag $CONTAINER_TEST_IMAGE $CONTAINER_RELEASE_IMAGE
    - docker push $CONTAINER_RELEASE_IMAGE
  only:
    - master

pages:
  stage: release
  script:
  - echo 'Nothing to do...'
  artifacts:
    paths:
    - public
  only:
  - master

#+END_SRC

*** Hoaxly Container Ports (and adapters)
    to talk to the other hoaxly containers
**** exposes port 6800 to allow scheduling spiders
     scrapyd, if running, can be interacted with 
**** uses port 9200 an 9300 to read from Storage Container via
     Elasticsearch via scrapyelasticsearch python library [3]
  
* Footnotes

[fn:1] http://portia.readthedocs.io/en/latest/spiders.html#running-a-spider


